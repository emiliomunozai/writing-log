{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Retriver functions\n",
    "\n",
    "Vector databases serve as key components for Retrieval-Augmented Generation (RAG) and semantic search applications. This submission aims to explain the different ways to retrieve vectors and how to improve these types of searches.\n",
    "\n",
    "There are three primary similarity functions used to compare vectors:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "Measures the straight-line distance between two vectors in a multi-dimensional space.\n",
    "Derived from the Pythagorean theorem.\n",
    "Works well when the absolute position of vectors is important, but it is sensitive to scale differences, which can be a drawback in text-based applications.\n",
    "2. Cosine Similarity\n",
    "Measures the angle between two vectors rather than their magnitude.\n",
    "Ranges from -1 (opposite directions) to 1 (same direction), with higher values indicating higher similarity.\n",
    "Works well for text-based embeddings since embeddings often have varying magnitudes but should be compared based on their direction in space.\n",
    "3. Dot Product\n",
    "Computes the projection of one vector onto another.\n",
    "Unlike cosine similarity, it considers both magnitude and direction, making it effective in models where vector magnitudes carry meaning.\n",
    "Commonly used in deep learning-based retrieval models but can be biased toward higher-magnitude vectors.\n",
    "Which Similarity Function is Best for RAG?\n",
    "For RAG applications, cosine similarity is often the preferred choice because:\n",
    "\n",
    "Text embeddings primarily encode meaning in vector directions rather than magnitudes.\n",
    "Cosine similarity normalizes for magnitude differences, making it more reliable for comparing text embeddings from models like OpenAIâ€™s Ada or Sentence-BERT.\n",
    "Many vector databases, including Pinecone and FAISS, optimize for cosine similarity in text retrieval tasks.\n",
    "However, in some cases, dot product similarity is also used, especially in transformer-based search models that rely on embedding magnitudes to capture additional meaning.\n",
    "\n",
    "Enhancing Retrieval Performance\n",
    "Beyond similarity functions, there are advanced techniques to improve retrieval accuracy, such as:\n",
    "\n",
    "Multi-vector search: Representing a query with multiple vectors instead of a single one. This is useful in handling complex queries with multiple semantic meanings.\n",
    "Hybrid search: Combining vector search with traditional keyword-based search (BM25) for better accuracy.\n",
    "Re-ranking techniques: Applying a second-stage ranking model, such as a cross-encoder, to refine the search results.s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
